# -*- coding: utf-8 -*-
"""botoRAG.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1L-crwhUVzKkAKJw3FEZdL-j6Te0AtLbD

### 1. Install requiered packages
"""

!nvidia-smi

!apt-get install -qq pciutils
!apt-get install -qq lshw

!pip install tqdm
!pip install transformers
!pip install langchain
!pip install langchain-community
!pip install pinecone-client
!pip install langchain-pinecone
!pip install sentence-transformers
!pip install langchain_ollama
!pip install colab-xterm

"""###2. Connect to a LLM"""

# Commented out IPython magic to ensure Python compatibility.
# %load_ext colabxterm
!curl -fsSL https://ollama.com/install.sh | sh
!ollama serve > ollama.log 2>&1 &
!sleep 10
!ollama pull llama3   # you can use llama3, phi3... any model available in https://ollama.com/library

"""###3. Import required tools

#### Packages
"""

from langchain_community.embeddings import HuggingFaceBgeEmbeddings
from langchain_community.document_loaders import PyMuPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.chains import ConversationalRetrievalChain
from langchain.memory import ConversationBufferMemory
from langchain.prompts import PromptTemplate
from langchain_pinecone import Pinecone
from langchain_community.llms import Ollama
from pinecone import Vector
from tqdm import tqdm
import subprocess
import pinecone
import time
import os
import warnings

warnings.filterwarnings("ignore")

"""####Embedding model"""

# Define an embedding model
embedding_model_name = 'sentence-transformers/all-mpnet-base-v2'
embedding_model = HuggingFaceBgeEmbeddings(model_name=embedding_model_name)

"""###3. App

####utils
"""

# DATABASE CREATION FUNCTIONS
# Function to upload and process pdf documents into a vectorized Database
def process_and_upsert_documents(docs_path, pnc_index, embedding_model):
  # intialize text splitter
  text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)

  # list of documents names
  docs_names = os.listdir(docs_path)

  # process documents
  for name in docs_names:
      if name.endswith('.pdf'):
          # create file paths
          file_path = os.path.join(docs_path, name)

          try:
              # upload pdf documents
              loader = PyMuPDFLoader(file_path)
              document = loader.load()

              # chunk documents
              chunks = text_splitter.split_documents(document)

              # create embedding vectors based on chunks
              vectors = []
              for i, chunk in enumerate(chunks):
                  vector_id = f"{name}-{i}"
                  vector_values = embedding_model.embed_documents([chunk.page_content])[0]
                  vector = Vector(
                      id=vector_id,
                      values=vector_values,
                      metadata=chunk.metadata
                  )
                  vectors.append(vector)

              # upsert vectors into the databse
              batch_size = 100
              total_vectors = len(vectors)
              for i in tqdm(range(0, total_vectors, batch_size), desc=f"Upserting {name}"):
                  batch = vectors[i:i+batch_size]
                  pnc_index.upsert(
                      vectors=batch,
                      namespace=name,
                      show_progress=False
                  )

              print(f'Document {name} was succesfully upload!')

          except Exception as e:
              print(f'Error in processing file: {name}: {e}')
  print("Your Vectorized Data Base was succesfully created!")



# CHAT LOGIC FUNCTIONS
# Function to create memory in chat
def create_memory():
    memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True)

    return memory


# Function to create conversation chain
def create_qa(llm, vectorstore, memory):
  qa = ConversationalRetrievalChain.from_llm(
  llm=llm,
  retriever=vectorstore.as_retriever(search_kwargs={"k": 5}),
  memory=memory
)

  return qa



# CHAT DISPLAY FUNCTIONS
# header
def header():
    print("Â¡Hello, I am Boto, your personal AI-Assistant! (Type 'exit' to quit)")


# user
def user_input():
    return input("You: ")


# bot
def bot_response(response):
    print("ðŸ¤– Assistant:", response)

# exit message
def if_exit(user_input):
    if user_input.lower() == "exit":
        print("Goodbye!")
        return True



# APP DISPLAY FUNCTIONS
# Function to give a welcome message
def welcome():
    print("Welcome to your personal AI-Assistant appðŸ¤–")


# Function to display an app menu
def options():
    options = ['Chat with Boto', 'Create a new DataBase', 'Exit']

    for number, option in enumerate(options, start=1):
        print(f'{number}. {option}')

    answer = None
    while answer is None:
        answer = input('What would you like to do?')
        try:
            answer = int(answer)
        except ValueError as e:
            answer = None
            print(f"{e}: Type (1) or (2) to select an option or (3) to exit")

        if answer == 1 or answer == 2 or answer == 3:
            os.system('cls')
            return answer
        else:
            answer = None
            print("Type (1) or (2) to select an option or (3) to exit")

"""####database"""

def create_DB(API_KEY_PINECONE, INDEX_NAME_PINECONE):
    # get documents path
    current_dir = os.getcwd()
    docs_path = os.path.join(current_dir, "documents")

    # Initialize pinecone DB
    try:
        pncDB = pinecone.Pinecone(api_key=API_KEY_PINECONE)
        index = pncDB.Index(INDEX_NAME_PINECONE)
    except Exception as e:
        print(f"An unexpected error occurred: {e}")

    process_and_upsert_documents(docs_path=docs_path,pnc_index=index)

"""####chat"""

def chat(API_KEY_PINECONE, INDEX_NAME_PINECONE, embedding_model):
    # Initialize pinecone DB
    try:
        pncDB = pinecone.Pinecone(api_key=API_KEY_PINECONE)
        index = pncDB.Index(INDEX_NAME_PINECONE)
    except Exception as e:
        print(f"An unexpected error occurred: {e}")

    # Initialize phi3 llm
    system_message = "You are an AI assistant that helps to write my thesis. Your name is Boto and always respond only in English lenguage."
    llm = Ollama(model="llama3", temperature=0.7, system=system_message)

    # Initialize vector store
    vectorstore = Pinecone(index, embedding_model, "text")

    # Create chat memory
    memory = create_memory()

    # Create conversation chain in chat
    qa = create_qa(llm=llm, vectorstore=vectorstore, memory=memory)


    # Display header
    header()

    # Chat display
    while True:
        # user input
        input = user_input()

        # validate if user wants to exit
        exit = if_exit(user_input=input)
        if exit:
            break

        # get a bot responser
        result = qa({"question": input})
        response = result['answer']

        # display bot response
        bot_response(response=response)

        """
        if 'source_documents' in result:
            print("\nSources:")
            for doc in result['source_documents']:
                print(doc.page_content[:100] + "...")
        """

        print()

"""####app"""

def run_app(API_KEY_PINECONE, INDEX_NAME_PINECONE, embedding_model):
    # welcome message
    welcome()

    # app menu manager logic
    option = None
    while option is None:
        # display options menu
        option = options()

        if option == 1:
            chat(API_KEY_PINECONE, INDEX_NAME_PINECONE, embedding_model)
        elif option == 2:
            create_DB(API_KEY_PINECONE, INDEX_NAME_PINECONE)
        else:
            break

        option = None
        os.system('cls')

"""###4. Run"""

# Configure hugging face
os.environ["HUGGINGFACEHUB_API_TOKEN"] = "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"

# Connect to a pinecone DB
API_KEY_PINECONE ="ea466f40-b247-43cb-8d43-9d204b664aa0"
INDEX_NAME_PINECONE = "chatbot"

# Run chatbot app
run_app(API_KEY_PINECONE, INDEX_NAME_PINECONE, embedding_model)